{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JaJ6MeKU1Fkj"
      },
      "outputs": [],
      "source": [
        "# install the dependencies\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.1.3/spark-3.1.3-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.3-bin-hadoop3.2.tgz\n",
        "!pip -q install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.3-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.12:0.7.0 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell'\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "CXPebAfQAB6B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset:\n",
        "https://www.kaggle.com/datasets/ruchi798/data-science-job-salaries"
      ],
      "metadata": {
        "id": "B3HkvlNsIEK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show some settings of spark\n",
        "print(SparkConf().getAll())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tG7reGjFIEJ",
        "outputId": "2567c0cd-d0d7-4232-c806-b2d486a28d0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('spark.jars', 'file:///root/.ivy2/jars/io.delta_delta-core_2.12-0.7.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'), ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'), ('spark.repl.local.jars', 'file:///root/.ivy2/jars/io.delta_delta-core_2.12-0.7.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', '/root/.ivy2/jars/io.delta_delta-core_2.12-0.7.0.jar,/root/.ivy2/jars/org.antlr_antlr4-4.7.jar,/root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,/root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,/root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,/root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,/root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,/root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'), ('spark.submit.deployMode', 'client'), ('spark.files', 'file:///root/.ivy2/jars/io.delta_delta-core_2.12-0.7.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.antlr_ST4-4.0.8.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.name', 'pyspark-shell'), ('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# open specific file \n",
        "df = spark.read.csv('/lake/csv/ds_salaries.csv', header = True, inferSchema = True)"
      ],
      "metadata": {
        "id": "iT2iUW6XAPsi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open all csv files in the lake folder\n",
        "df = spark.read \\\n",
        "          .format('csv') \\\n",
        "          .option('inferSchema', 'true') \\\n",
        "          .option('header', 'true') \\\n",
        "          .load('/lake/csv/*.csv')"
      ],
      "metadata": {
        "id": "3TBQT5wiBATd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDYB34ZZDGaA",
        "outputId": "1be594af-c27f-43f4-ef23-cb8b8cdc1dcf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
            "|_c0|work_year|experience_level|employment_type|           job_title|salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|\n",
            "+---+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
            "|  0|     2020|              MI|             FT|      Data Scientist| 70000|            EUR|        79833|                DE|           0|              DE|           L|\n",
            "|  1|     2020|              SE|             FT|Machine Learning ...|260000|            USD|       260000|                JP|           0|              JP|           S|\n",
            "|  2|     2020|              SE|             FT|   Big Data Engineer| 85000|            GBP|       109024|                GB|          50|              GB|           M|\n",
            "|  3|     2020|              MI|             FT|Product Data Analyst| 20000|            USD|        20000|                HN|           0|              HN|           S|\n",
            "|  4|     2020|              SE|             FT|Machine Learning ...|150000|            USD|       150000|                US|          50|              US|           L|\n",
            "+---+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save csv\n",
        "df.write.format('csv') \\\n",
        "        .mode('overwrite') \\\n",
        "        .partitionBy(\"experience_level\") \\\n",
        "        .save(\"/home\")"
      ],
      "metadata": {
        "id": "yczRTV2fFK5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save parquet\n",
        "df.write.format('parquet') \\\n",
        "        .mode('overwrite') \\\n",
        "        .partitionBy('employee_residence',\"experience_level\") \\\n",
        "        .save(\"/lake/parquet\")"
      ],
      "metadata": {
        "id": "8hNh3rYi3HjW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open parquet\n",
        "df = spark.read.format('parquet').load('/lake/parquet')"
      ],
      "metadata": {
        "id": "RMBlxD8rDTdc"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save orc\n",
        "df.write.format('orc').mode('overwrite').save('/lake/orc')"
      ],
      "metadata": {
        "id": "_9I4hjmPDVpb"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save delta\n",
        "df.write.format('delta').mode('overwrite').save('/lake/delta')"
      ],
      "metadata": {
        "id": "rf7DJrhVAPqF"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open delta using the versionAsOf\n",
        "df= spark.read.format('delta').option('versionAsOf', '0').load('/lake/delta')"
      ],
      "metadata": {
        "id": "aXS1pEsHGejC"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding duplicates data in the delta table and reading using Time Travel feature with timestampAsOf\n",
        "df.write.format('delta').mode('append').save('/lake/delta')\n",
        "df = spark.read.format('delta').option('timestampAsOf', '2022-11-07 12:40').load('/lake/delta')"
      ],
      "metadata": {
        "id": "xPaDrAfWEI0g"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the table was create in the 2022-11-07 12:38:46.635"
      ],
      "metadata": {
        "id": "XiZQZffvHzk4"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B1UVcKhxKhq8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}